import streamlit as st
import pandas as pd
import json


# --- 1. EVALUATION ENGINE ---
def calculate_veregin_score(inferred_val, reference_val, field_name):
    """Calculates Veregin's Matrix (0-2 scale) for a metadata field."""
    scores = {"Correctness": 0, "Completeness": 0, "Consistency": 0, "Granularity": 0}

    # 1. Completeness
    if inferred_val is not None and inferred_val != "Unknown" and inferred_val != []:
        scores["Completeness"] = 2

    # 2. Correctness (Match check)
    if str(inferred_val).strip().lower() == str(reference_val).strip().lower():
        scores["Correctness"] = 2
    elif str(inferred_val).lower() in str(reference_val).lower():
        scores["Correctness"] = 1

    # 3. Consistency (Logic check)
    # Example: Check if Bounding Box coordinates are mathematically logical
    if field_name == "geographic_extent" and isinstance(inferred_val, dict):
        if inferred_val.get("min_x", 0) < inferred_val.get("max_x", 0):
            scores["Consistency"] = 2
    else:
        scores["Consistency"] = 2

    # 4. Granularity (Detail check)
    # Example: Check if geometry is specific (MultiPolygon) or generic (Vector)
    if field_name == "geometry_type" and any(x in str(inferred_val).upper() for x in ['MULTI', 'POINT', 'LINE']):
        scores["Granularity"] = 2
    else:
        scores["Granularity"] = 1

    scores["Total"] = sum(scores.values())
    return scores


# --- 2. SIDEBAR: REFERENCE FILE UPLOAD ---
st.sidebar.markdown("---")
st.sidebar.header("Evaluation Reference")
ref_file = st.sidebar.file_uploader("Upload Reference Metadata (JSON/MID)", type=['json', 'mid'])

# --- 3. MAIN LOGIC (Within your existing file processing loop) ---
# Assuming 'metadata' is the dictionary generated by your inference methods...

if uploaded_file is not None:
    # ... (Your existing code for previewing data and generating 'metadata') ...

    if ref_file is not None:
        try:
            # Load the reference data
            reference_data = json.load(ref_file)

            st.divider()
            st.header("âš–ï¸ Vereginâ€™s Matrix Evaluation")
            st.info("Comparing Inferred Metadata against Reference Standard")

            # Define which fields to compare
            # Note: Ensure your reference JSON keys match these field names
            comparison_map = {
                "geometry_type": (metadata.get("geometry_info", {}).get("type"), reference_data.get("geometry_type")),
                "total_features": (metadata.get("total_features"), reference_data.get("total_features")),
                "geographic_extent": (metadata.get("geographic_extent"), reference_data.get("geographic_extent"))
            }

            eval_report = []
            for field, values in comparison_map.items():
                inferred, reference = values
                field_scores = calculate_veregin_score(inferred, reference, field)
                field_scores["Field"] = field
                eval_report.append(field_scores)

            # --- 4. DISPLAY EVALUATION RESULTS ---
            eval_df = pd.DataFrame(eval_report).set_index("Field")

            # Highlight scores in a table
            st.dataframe(eval_df.style.background_gradient(cmap="RdYlGn", subset=["Total"], low=0, high=8))

            # Method-level aggregation
            total_score = eval_df["Total"].sum()
            max_possible = len(eval_report) * 8
            performance_ratio = (total_score / max_possible) * 100

            c1, c2, c3 = st.columns(3)
            c1.metric("Overall Score", f"{total_score} / {max_possible}")
            c2.metric("Accuracy Rate", f"{performance_ratio:.1f}%")

            # Qualitative Error Analysis
            with st.expander("ðŸ” Detailed Error Analysis"):
                for index, row in eval_df.iterrows():
                    if row["Total"] < 6:
                        st.write(
                            f"**{index}:** Low score detected. Check if the inference method is too coarse for this dataset.")

        except Exception as e:
            st.error(f"Error reading reference file: {e}")
    else:
        st.warning("Please upload a reference JSON/MID file in the sidebar to perform ISO evaluation.")